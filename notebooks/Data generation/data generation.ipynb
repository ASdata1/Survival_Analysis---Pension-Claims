{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71eb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from lifelines import KaplanMeierFitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d46bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 5000  # number of individuals\n",
    "\n",
    "# --- 1. Covariate generation ----------------------------------------------\n",
    "age_at_entry = np.random.randint(25, 60, n) \n",
    "income_level = np.random.normal(40000, 12000, n).clip(15000, 120000)\n",
    "health_score = np.random.normal(70, 10, n).clip(40, 100)\n",
    "pension_contrib_rate = np.random.uniform(0.03, 0.12, n)\n",
    "\n",
    "#  2. Weibull AFT parameters ----------------------------------------\n",
    "k = 1.8  # shape parameter >1 → hazard increases over time\n",
    "\n",
    "# regression coefficients for log(scale λ_i)\n",
    "beta_age = -0.015      # older = earlier claim\n",
    "beta_income = 0.000015 # higher income = later claim\n",
    "beta_health = 0.01     # better health = later claim\n",
    "beta_contrib = -2.0    # higher contrib rate = earlier claim\n",
    "mu = 2.4             # intercept (base scale level)\n",
    "\n",
    "# 3. Individual scale parameter λ_i ----------------------------------------\n",
    "# log-linear model for scale parameter\n",
    "log_lambda_i = (\n",
    "    mu\n",
    "    + beta_age * age_at_entry\n",
    "    + beta_income * income_level\n",
    "    + beta_health * health_score\n",
    "    + beta_contrib * pension_contrib_rate\n",
    ")\n",
    "lambda_i = np.exp(log_lambda_i)\n",
    "\n",
    "# --- 4. True event times (Weibull distributed) -----------------------------\n",
    "true_time = np.random.weibull(k, n) * lambda_i  # years until claim\n",
    "\n",
    "# --- 5. INFORMATIVE Right censoring (make censoring depend on covariates) ---\n",
    "# Instead of random censoring, make censoring strongly related to covariates\n",
    "\n",
    "# REASONING: Create strong covariate-dependent censoring to demonstrate IPCW effectiveness.\n",
    "# In real pension data, censoring is rarely random - certain types of people systematically\n",
    "# leave the study early (job changes, relocations, plan switches). By making censoring\n",
    "# highly predictable from age, income, health, and contribution patterns, we create a \n",
    "# scenario where naive methods (zero/discard) will be biased, but IPCW can correct for\n",
    "# these systematic differences. This allows us to showcase when and why IPCW provides\n",
    "# substantial improvements over simpler censoring approache\n",
    "# Base censoring risk \n",
    "base_censoring_risk = 0.3\n",
    "\n",
    "# Strong censoring effects - make censoring highly predictable from covariates\n",
    "gamma_age = -0.05      # younger people more likely to be censored (leave jobs)\n",
    "gamma_income = 0.00005 # higher income = less likely to be censored (stable jobs) \n",
    "gamma_health = 0.02    # healthier people more likely to be censored (change jobs)\n",
    "gamma_contrib = -5.0   # higher contributors less likely to be censored (committed)\n",
    "\n",
    "# Calculate individual censoring probability based on covariates\n",
    "censoring_linear_pred = (\n",
    "    base_censoring_risk\n",
    "    + gamma_age * age_at_entry\n",
    "    + gamma_income * income_level  \n",
    "    + gamma_health * health_score\n",
    "    + gamma_contrib * pension_contrib_rate\n",
    ")\n",
    "\n",
    "# Convert to probabilities (sigmoid transformation)\n",
    "censoring_prob = 1 / (1 + np.exp(-censoring_linear_pred))\n",
    "\n",
    "# Generate censoring times based on individual risk\n",
    "min_censor_time = 2.0   # minimum 2 years before censoring\n",
    "max_censor_time = 25.0  # maximum 25 years\n",
    "\n",
    "# High censoring probability = early censoring time\n",
    "# Low censoring probability = late censoring time  \n",
    "censor_time = min_censor_time + (max_censor_time - min_censor_time) * (1 - censoring_prob)\n",
    "\n",
    "# Add randomness to censoring times\n",
    "censor_time += np.random.normal(0, 1.5, n)  #  create small amount of noise so model predicts are not perfect\n",
    "censor_time = np.clip(censor_time, min_censor_time, max_censor_time)\n",
    "\n",
    "# Observed time & event indicator\n",
    "observed_time = np.minimum(true_time, censor_time)\n",
    "event_observed = (true_time <= censor_time).astype(int)\n",
    "# --- 6. Combine into a DataFrame ------------------------------------------\n",
    "df = pd.DataFrame({\n",
    "    \"id\": np.arange(1, n + 1),\n",
    "    \"age_at_entry\": age_at_entry,\n",
    "    \"income_level\": income_level.round(2),\n",
    "    \"health_score\": health_score.round(1),\n",
    "    \"pension_contrib_rate\": pension_contrib_rate.round(3),\n",
    "    \"true_event_time\": true_time.round(3),\n",
    "    \"censor_time\": censor_time.round(3),\n",
    "    \"time_to_event\": observed_time.round(3),\n",
    "    \"event_observed\": event_observed\n",
    "})\n",
    "\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd0c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 7. Save to CSV --------------------------------------------------------\n",
    "df.to_csv(\"synthetic_survival_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3d0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the DataFrame\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fcec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 8. Visualize distributions of true event times and censoring times ----\n",
    "df_copy = df.copy()\n",
    "df_copy['True_Time'] = true_time.round(2)\n",
    "df_copy['time_to_event'] = censor_time.round(2)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(df_copy['True_Time'], bins=50, alpha=0.5, label='True Event Times', color='blue')\n",
    "plt.hist(df_copy['time_to_event'], bins=50, alpha=0.5, label='Censoring Times', color='orange')\n",
    "plt.xlabel('Time (years)')  \n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of True Event Times and Censoring Times')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- 10. Analyze censoring pattern -----------------------------------------\n",
    "print(\"Analyzing censoring pattern\")\n",
    "\n",
    "# Load and check the original data\n",
    "print(f\"Total observations: {len(df)}\")\n",
    "print(f\"Events: {(df['event_observed'] == 1).sum()}\")\n",
    "print(f\"Censored: {(df['event_observed'] == 0).sum()}\")\n",
    "print(f\"Censoring rate: {(df['event_observed'] == 0).mean():.3f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df[df['event_observed'] == 1]['time_to_event'], \n",
    "         alpha=0.7, label='Events', bins=30)\n",
    "plt.hist(df[df['event_observed'] == 0]['time_to_event'], \n",
    "         alpha=0.7, label='Censored', bins=30)\n",
    "plt.xlabel('Time to Event')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Events vs Censored by Time')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Calculate survival function for censoring\n",
    "\n",
    "km_c = KaplanMeierFitter()\n",
    "km_c.fit(df[\"time_to_event\"], 1 - df[\"event_observed\"])  # 1=censored\n",
    "km_c.plot(label='Censoring Survival Function')\n",
    "plt.title('Probability of Not Being Censored Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('P(Not Censored)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check if censoring is informative - that they are related to the covariates used \n",
    "print(\"\\nCensoring by covariates:\")\n",
    "for col in ['age_at_entry', 'income_level', 'health_score', 'pension_contrib_rate']:\n",
    "    censored_mean = df[df['event_observed'] == 0][col].mean()\n",
    "    event_mean = df[df['event_observed'] == 1][col].mean()\n",
    "    print(f\"{col}: Censored={censored_mean:.3f}, Events={event_mean:.3f}, Diff={abs(censored_mean-event_mean):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
